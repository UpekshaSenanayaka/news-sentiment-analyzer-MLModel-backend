{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5edc9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a706d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vidyapani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vidyapani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords and punkt tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d70a4d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vidyapani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the WordNet lemmatizer data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b80d4020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\vidyapani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the vader lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e662346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uplaod the data from your local directory\n",
    "df = pd.read_csv('data_collection.csv',encoding='latin1')\n",
    "\n",
    "# Remove rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Read the news headlines from a CSV file and drop any duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b889ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text by removing stop words, punctuation, and converting to lowercase\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text.lower())\n",
    "    # add lemmatizer\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]\n",
    "    # Remove stopwords and punctuation\n",
    "    words = [w for w in words if w not in stop_words and re.match(r'[^\\W\\d]', w)]\n",
    "    # Join the remaining words back into a string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2dc32c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    14066\n",
       "positive     9750\n",
       "neutral      6658\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the preprocess function with a polarity threshold of 0.2\n",
    "\n",
    "def preprocess(text, polarity_threshold=0.1):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    polarity = scores['compound']\n",
    "\n",
    "    if polarity > polarity_threshold:\n",
    "        sentiment = 'positive'\n",
    "    elif polarity < -polarity_threshold:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "# Apply the preprocessing function to the headline column\n",
    "df['headline'] = df['headline_text'].apply(preprocess_text)\n",
    "\n",
    "# Apply the preprocess function to each row of the preprocessed headline column\n",
    "df['sentiment'] = df['headline'].apply(preprocess)\n",
    "\n",
    "#find the label counts \n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69935b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74789c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using a bag-of-words model\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50b57cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a SVC classifier on the training data\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d46727d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing data: 0.9340442986054143\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier on the testing data and print the accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy on testing data:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aacb189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.9869559867098733\n"
     ]
    }
   ],
   "source": [
    "# Predict the sentiment of the training data and print the accuracy\n",
    "y_pred_train = clf.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print('Accuracy on training data:', accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf60a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: neutral\n"
     ]
    }
   ],
   "source": [
    "# Predict the sentiment of new input text\n",
    "input_text = \"Stock market closes with minimal change after mixed earnings reports\"\n",
    "preprocessed_text = preprocess_text(input_text)\n",
    "vectorized_text = vectorizer.transform([preprocessed_text])\n",
    "prediction = clf.predict(vectorized_text)\n",
    "print('Prediction:', prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b054a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "476321ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask-cors in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: Flask>=0.9 in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (from flask-cors) (2.0.3)\n",
      "Requirement already satisfied: Six in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (from flask-cors) (1.16.0)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (from Flask>=0.9->flask-cors) (3.1.2)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (from Flask>=0.9->flask-cors) (2.0.3)\n",
      "Requirement already satisfied: click>=7.1.2 in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (from Flask>=0.9->flask-cors) (8.0.4)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (from Flask>=0.9->flask-cors) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (from click>=7.1.2->Flask>=0.9->flask-cors) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vidyapani\\anaconda3\\envs\\keras_env\\lib\\site-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.1)\n",
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [03/Mar/2023 09:58:22] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 09:58:22] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 09:58:30] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 09:58:30] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 09:58:39] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 09:58:39] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:01:24] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:01:24] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:01:57] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:01:57] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:03:24] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:03:24] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:04:24] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:04:24] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:04:49] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:04:49] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:05:39] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:05:39] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:06:16] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:06:16] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:06:44] \"OPTIONS /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Mar/2023 10:06:44] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "!pip install -U flask-cors\n",
    "import flask\n",
    "import io\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from flask import Flask, jsonify, request \n",
    "import joblib\n",
    "from flask import Flask\n",
    "from flask_cors import CORS\n",
    "\n",
    "# saving our model\n",
    "joblib.dump(clf , 'model_jlib')\n",
    "\n",
    "# opening the file\n",
    "m_jlib = joblib.load('model_jlib')\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# define the API endpoint\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # receive the input data from the request\n",
    "    input_data = request.get_json().get(\"data\")\n",
    "\n",
    "    # make predictions using the loaded model \n",
    "    try:\n",
    "        preprocessed_text = preprocess_text(input_data)\n",
    "        vectorized_text = vectorizer.transform([preprocessed_text])\n",
    "        prediction = clf.predict(vectorized_text)\n",
    "        return jsonify({'prediction': prediction[0]})\n",
    "        \n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e2665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
